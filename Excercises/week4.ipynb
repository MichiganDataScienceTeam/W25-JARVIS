{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4: Setting Up and using Ollama for a Locally Hosted LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Install Required Dependencies\n",
    "\n",
    "(we've used these before, but just to make sure): \n",
    "\n",
    "- Python 3.8+\n",
    "- Jupyter Notebook\n",
    "- Ollama (instructions below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.22.0)\n",
      "Collecting httpx<0.29,>=0.27 (from ollama)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting anyio (from httpx<0.29,>=0.27->ollama)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<0.29,>=0.27->ollama) (2019.11.28)\n",
      "Collecting httpcore==1.* (from httpx<0.29,>=0.27->ollama)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<0.29,>=0.27->ollama) (2.8)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.29,>=0.27->ollama)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/aarushi/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<0.29,>=0.27->ollama)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, h11, annotated-types, pydantic, httpcore, anyio, httpx, ollama\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 ollama-0.4.7 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required Python libraries\n",
    "!pip install ollama requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Setup Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama is a tool for running LLMs locally. Follow these steps to set it up:\n",
    "\n",
    "1. Download Ollama:\n",
    "\n",
    "* Visit the Ollama website and download the latest release for your operating system: https://ollama.com/download\n",
    "\n",
    "* Alternatively, use the command line to install it if you are using linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run Ollama:\n",
    "\n",
    "* Start the Ollama server locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama serve # make sure you are running this in a terminal! might take a second to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2990350625.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ollama serve # starts the server, which will handle requests to the LLM\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ollama serve # starts the server, which will handle requests to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Download a Model:\n",
    "\n",
    "* Ollama supports various LLMs (e.g., LLaMA, GPT-J). For today, we chose to use llama3.1 since it is good for a personal assistant application. Gemma 2 9b is also another good model for this use case.\n",
    "\n",
    "Download the llam3.1 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3062653468.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ollama run llama3.1:8b-instruct-q6_K\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "ollama run llama3.1:8b-instruct-q6_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Verify Ollama Setup\n",
    "Ensure Ollama is running and accessible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Test the Ollama API\n",
    "response = requests.post(\"http://localhost:8080/api/generate\", json={\"model\": \"llama3.1\", \"prompt\": \"Hello, world!\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Implementing it into JARVIS\n",
    "Now that Ollama is set up, let's create a simple JARVIS-like assistant.\n",
    "\n",
    "1. Define the Assistant Logic:\n",
    "\n",
    "* Use Ollama's API to generate responses based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jarvis(prompt):\n",
    "    # Send the prompt to Ollama\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8080/api/generate\",\n",
    "        json={\"model\": \"llama2\", \"prompt\": prompt}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "    else:\n",
    "        return \"Error: Unable to generate a response.\"\n",
    "\n",
    "# Test the JARVIS function\n",
    "print(jarvis(\"What is the weather like today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add Voice Input/Output:\n",
    "\n",
    "We used the speech recognition and text-to-speech libraries previously to enable voice interation. We'll be integrating them again to use them with our model. Feel free to look back on that code, and reuse it here. \n",
    "\n",
    "* Use libraries like speech_recognition and pyttsx3 to enable voice interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def listen():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"You said: {text}\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Sorry, I didn't catch that!\"\n",
    "\n",
    "def speak(text):\n",
    "    print(f\"JARVIS: {text}\")\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# test voice interaction\n",
    "user_input = listen()\n",
    "response = jarvis(user_input)\n",
    "speak(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Create a Flask Web Interface (If extra time, otherwise we will go more in depth in the next few weeks)\n",
    "To make JARVIS accessible via a web interface, use Flask. You'll need to run the following first if you don't have flask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# create a Flask app instance\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/jarvis\", methods=[\"POST\"]) # define a route for the JARVIS endpoint\n",
    "\n",
    "def jarvis_endpoint():\n",
    "    # get JSON data from the request\n",
    "    data = request.json\n",
    "\n",
    "    # extract the \"prompt\" from the JSON data (default to empty string if not provided)\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "\n",
    "    # call the JARVIS function with the prompt\n",
    "    response = jarvis(prompt)\n",
    "\n",
    "    # return the response as JSON again\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "# lastly, run the Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
