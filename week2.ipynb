{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "Today's focus in going to be on creating the LLM portion of our API-driven program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "For our LLM, we will be using the PerplexityAPI. Sign in with your umich account for free acess to Perplexity Pro.\n",
    "\n",
    "Once logged in, clicked the setting button in the bottom left, then \"API\" on the top right.\n",
    "\n",
    "Scroll down to find your API-key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using and API key, we should define it in a ```.env``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPLX_API_KEY = \"<INSERT KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load our API key into any one of our scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PPLX_API_KEY = os.getenv(\"PPLX_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our API requests, we can use our generated key to complete requests.\n",
    "A template for how to do so can be found here: https://docs.perplexity.ai/guides/getting-startedhttps://docs.perplexity.ai/guides/getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use the provided link to guide you in making an API call\n",
    "\n",
    "# HINT: to isolate the content of the response, return: response.choices[0].message.content\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've got the model working, try playing around with different parameters, whether it be the system content or model type.\n",
    "Supported models for the Perplexity API can be found here: https://docs.perplexity.ai/guides/model-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "If you are satisfied with the model, it's time to run some tests. Play around and try to identify the following...\n",
    "- Does the LLM fail to accurately respond to some prompts?\n",
    "- How fast does the LLM respond?\n",
    "- Is the LLM consistent with its responses?\n",
    "\n",
    "Feel free to explore any number of tests that come to mind. \n",
    "\n",
    "Once we have completed our locally hosted version of JARVIS, we can revisit these tests and rerun them on both the API version and locally hosted version. This way, we can come up with numerical data to compare our two programs.\n",
    "\n",
    "If interested, feel free to run these tests on the full program workflow: stt -> LLM -> tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create and run some tests for the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting LLM\n",
    "\n",
    "One downfall of many LLMs is their tendency to hallucinate. To address this issue, we can use techniques functioning calling and RAG.\n",
    "\n",
    "Unfortunatley, Perplexity does not provide support for these techniques. However, other popular LLM APIs, such as the OpenAI API, do. \n",
    "\n",
    "To do this, we can use libraries like LangChain and LlamaIndex to create AI agents.\n",
    "For means of excercise, we can attempt to replicate the effect function calling using LangChain and the PerplexityAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: content='CALL magic_function(3)' additional_kwargs={'citations': ['https://python.langchain.com/docs/how_to/migrate_agent/', 'https://edmcman.github.io/blog/', 'https://switowski.com/blog/creating-magic-functions-part3/', 'https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling', 'https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html']} response_metadata={} id='run-133d4c5f-9d3c-4843-a954-efb40d320d58-0'\n",
      "The function returned: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatPerplexity\n",
    "\n",
    "# Step 1: Initialize model and helper function\n",
    "model = ChatPerplexity(\n",
    "\n",
    ")\n",
    "\n",
    "def magic_function():\n",
    "    return True\n",
    "\n",
    "# Step 2: Define initial query and invoke response\n",
    "# HINT: state the helper function (including paramters) you would like used in your query\n",
    "query = \"\"\n",
    "\n",
    "# HINT: use the f-string to include the user query in the response\n",
    "response = model.invoke(f\"\"\"\n",
    "    < PROVIDE SYSTEM CONTENT > \n",
    "    You are an AI.\n",
    "                        \n",
    "    User Query: \n",
    "\"\"\")\n",
    "\n",
    "print(\"Model Response:\", response)\n",
    "\n",
    "# Step 3: Extract function request\n",
    "# HINT: use the regex library to re.search the function call in the initial query; \n",
    "    # if found, manually define the output, otherwise return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calling and RAG are powerful tools to improve the performance of an LLM. In future weeks, we may return to these ideas when creating our locally hosted program. \n",
    "\n",
    "That said, if you are interested in exploring how these techniques may be implemented using an OpenAI API key, consider taking a look at the LangChain documentation:\n",
    "- RAG: https://python.langchain.com/docs/tutorials/rag/\n",
    "- LangChain Tools: https://python.langchain.com/docs/how_to/custom_tools/#creating-tools-from-functions\n",
    "- LangChain/LangGraph Agents: https://python.langchain.com/docs/how_to/migrate_agent/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
