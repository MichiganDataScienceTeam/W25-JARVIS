{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4: Setting Up and using Ollama for a Locally Hosted LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Install Required Dependencies\n",
    "\n",
    "(we've used these before, but just to make sure): \n",
    "\n",
    "- Python 3.8+\n",
    "- Jupyter Notebook\n",
    "- Ollama (instructions below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required Python libraries\n",
    "!pip install ollama requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Setup Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama is a tool for running LLMs locally. Follow these steps to set it up:\n",
    "\n",
    "1. Download Ollama:\n",
    "\n",
    "* Visit the Ollama website and download the latest release for your operating system: https://ollama.com/download\n",
    "\n",
    "* Alternatively, use the command line to install it (if available).\n",
    "\n",
    "2. Run Ollama:\n",
    "\n",
    "* Start the Ollama server locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama serve # starts the server, which will handle requests to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Download a Model:\n",
    "\n",
    "* Ollama supports various LLMs (e.g., LLaMA, GPT-J). For today, we chose to use llama3.1 since it is good for a personal assistant application. Gemma 2 9b is also another good model for this use case.\n",
    "\n",
    "Download the llam3.1 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama run llama3.1:8b-instruct-q6_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Verify Ollama Setup\n",
    "Ensure Ollama is running and accessible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Test the Ollama API\n",
    "response = requests.post(\"http://localhost:8080/api/generate\", json={\"model\": \"llama3.1\", \"prompt\": \"Hello, world!\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Implementing it into JARVIS\n",
    "Now that Ollama is set up, let's create a simple JARVIS-like assistant.\n",
    "\n",
    "1. Define the Assistant Logic:\n",
    "\n",
    "* Use Ollama's API to generate responses based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jarvis(prompt):\n",
    "    # Send the prompt to Ollama\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8080/api/generate\",\n",
    "        json={\"model\": \"llama2\", \"prompt\": prompt}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "    else:\n",
    "        return \"Error: Unable to generate a response.\"\n",
    "\n",
    "# Test the JARVIS function\n",
    "print(jarvis(\"What is the weather like today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add Voice Input/Output:\n",
    "\n",
    "We used the speech recognition and text-to-speech libraries previously to enable voice interation. We'll be integrating them again to use them with our model. Feel free to look back on that code, and reuse it here. \n",
    "\n",
    "* Use libraries like speech_recognition and pyttsx3 to enable voice interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def listen():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"You said: {text}\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Sorry, I didn't catch that!\"\n",
    "\n",
    "def speak(text):\n",
    "    print(f\"JARVIS: {text}\")\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# test voice interaction\n",
    "user_input = listen()\n",
    "response = jarvis(user_input)\n",
    "speak(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Create a Flask Web Interface (If extra time, otherwise we will go more in depth in the next few weeks)\n",
    "To make JARVIS accessible via a web interface, use Flask. You'll need to run the following first if you don't have flask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# create a Flask app instance\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/jarvis\", methods=[\"POST\"]) # define a route for the JARVIS endpoint\n",
    "\n",
    "def jarvis_endpoint():\n",
    "    # get JSON data from the request\n",
    "    data = request.json\n",
    "\n",
    "    # extract the \"prompt\" from the JSON data (default to empty string if not provided)\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "\n",
    "    # call the JARVIS function with the prompt\n",
    "    response = jarvis(prompt)\n",
    "\n",
    "    # return the response as JSON again\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "# lastly, run the Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
